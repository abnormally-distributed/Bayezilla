---
title: "R Notebook"
output: html_notebook
---

Load the package..
```{r}
library(Bayezilla)
```


```{r eval=FALSE, include=FALSE}
# Simulate data
library(fBasics)
library(gdata)
set.seed(101)  # for reproducibility
N <- 20
P <- 10
#True betas
betas = c(1.9927551, 0, .353434, 0, 0, -1.18113776, -0.9273929, 0.72321414, 0.13034, 0) 
#groupings = c(rep(1, 5), rep(2, 5), rep(3, 4))
#wch = sample(1:P, size= length(temp))
#betas[wch] <- temp
#rm(temp)
#idx = rep(0, P)
#idx[wch] <- groupings
#idx[which(idx == 0)] <- seq(4, 34)
mu <- 0
#Generate Covariance Matrix
#Generate data simulation
Sigma = jitter(diag(rep(1, P)), amount = .95)
neg = which(Sigma < -1)
pos = which(Sigma > 1)
Sigma[neg] <- -1
Sigma[pos] <- 1
upperTriangle(Sigma) <- lowerTriangle(Sigma, byrow = TRUE)
diag(Sigma) <- 1
Sigma = makePositiveDefinite(as.matrix(Sigma))
diag(Sigma) <- 1
Sigma = makePositiveDefinite(Sigma)
Sigma = cov2cor(Sigma)
set.seed(101)
X = MASS::mvrnorm(N, mu = rep(0, P), Sigma = Sigma, empirical = FALSE)
X = as.matrix(scale(X))
y = mu + as.vector(X %*% matrix(betas, ncol = 1))
X = X + matrix(rnorm((N * P)/4 , .0, .0001), N, P)
#noise = rgamma((N * P)/4 , 2, 2)
#noise = c(scale(noise), rev(scale(noise)), scale(noise), -1 * scale(noise)) * .62
#X = X + matrix(noise, nrow = N, ncol = P)
colnames(X) = paste0("X", 1:P)
# Add gaussian noise
data = cbind.data.frame(Y = y, X)
colmeans  = colMeans(data)
colsds = colSds(data)
```

```{r eval=FALSE, include=FALSE}
data = readr::read_csv("simulation_data.csv")
```

Let's first take a look at the classical linear model.

```{r echo=FALSE}
broom::tidy(lm(Y ~ ., data)) %>% mutate_if(is.numeric, round, 3)
lmcoefs = broom::tidy(lm(Y ~ ., data)) %>% mutate_if(is.numeric, round, 3)
lmcoefs = lmcoefs$estimate
```

Next, let's try a Bayesian linear regression using the Bayezilla package. This model does not require a subjective specification of the scales of the coefficient priors -- they are learned from the data through a gamma(.5, .5) hyperprior which makes the marginal priors Cauchy distributed. Functionally this operates in a similar manner to ridge regression. Hence, this is a fully data driven upgrade to a standard GLM function. I will utilize 6000 samples per chain. Normally I run at least 10000 per chain regardless but for demonstration purposes 6000 samples per chain is perfectly adequate. 

```{r}
out = glm_bayes(Y ~ ., data, log_lik = TRUE, method = "rjparallel", cl = makeCluster(4), iter = 10000, thin = 2, chains = 4)
```

Now let's examine the summary of the model. The model summary contains an estimate, which is the expected value (mean) of the marginal posterior distribution of the variable. Next to this column is the median of the marginal posteriors, followed by the standard error (in Bayesian estimation the standard error is the standard deviation of the posterior distribution) and 90% equal-tailed quantile intervals. Last is a measure of the *effective sample size* to ensure the model was sampled adequately (a good number to aim for is above 4000 for an accurate measure of the mean and median estimates, and at least 10000 for 95 % intervals to ensure accuracy up to 2 decimal places for the endpoints. 90% intervals can make do with ~6000 or so). These features are customizable, but for these examples I will leave them as the default. 

```{r echo=FALSE}
post_summary(out)
coefs1 = post_summary(out)$estimate[2:11]
```

Now let's take a look at the most basic form of Bayesian variable selection. This samples the joint model-parameter space by using indicator variables to sample which
coefficients are active in each model sampled. In addition to sampling models, different parameter values are explored. This is a notable improvement over methods such as stepwise regression or classical subsets regression which do not account for parameter uncertainty. 


```{r}
out2 = glm_spike(Y ~ ., data, method = "rjparallel", cl = makeCluster(4), log_lik = TRUE, iter = 10000, thin = 3, adapt = 5000, warmup = 5000, chains = 4)
```


```{r}
post_summary(out2)
coefs2 = post_summary(out2)$estimate[2:11]
```

An alternative to the Bayesian Model Averaging / Spike-And-Slab approach to variable selection is the family of LASSO estimators. One reason you might consider a LASSO family estimator is that the BMA approach does not scale well for large data sets (large P or large N) for computatioanl reasons. Sampling a mixture of discrete and continuous variables as in the Bernoulli-Normal mixtures the BMA algorithm uses is a burdensome task on current technology. Furthermore, as P grows the model space increases at a rate of $2^P$, and on top of that, uncertainty in each coefficient must be explored. This quickly adds up to an increasingly difficult high dimensional space for the MCMC chains to explore. LASSO on the other hand uses continuous shrinkage priors, which scale much better to bigger problems. 

```{r}
out3 = extLASSO(Y ~ ., data, method = "rjparallel", cl = makeCluster(4), log_lik = TRUE, iter = 15000, adapt = 5000, thin = 3)
```

As you can see, the results are quite consistent with the above results. 

```{r}
post_summary(out3)
coefs3 = post_summary(out3)$estimate[2:11]
```


```{r}
out4 = negLASSO(Y ~ ., data, method = "rjparallel", cl = makeCluster(4), log_lik = TRUE, iter = 10000, adapt = 5000, thin = 3)
```

```{r}
post_summary(out4)
coefs4 = post_summary(out4)$estimate[2:11]
```




```{r}
out5 = bayesEnet(Y ~ ., data, method = "rjparallel", cl = makeCluster(4), log_lik = TRUE, iter = 15000, warmup = 2500, adapt = 5000,  thin = 4)
```


```{r}
post_summary(out5)
coefs5 = post_summary(out5)$estimate[2:11]
```






```{r}
true.betas = round(c(1.9927551, 0, .353434, 0, 0, -1.18113776, -0.9273929, 0.72321414, 0.13034, 0) , 3)
coefTable = data.frame(
trueBetas = true.betas,
OLS = lmcoefs[-1],
BayesGLM = coefs1,
normalBernoulliMixture = coefs2,
extendedBLASSO = coefs3,
negBLASSO = coefs4,
BayesElasticNet = coefs5
)

coefTable

EstimationError = cbind.data.frame(
  OLS = sum(abs(lmcoefs[-1] - true.betas)),
  BayesGLM = sum(abs(coefs1 - true.betas)),
  normalBernoulliMixture = sum(abs(coefs2 - true.betas)),
  extendedBLASSO = sum(abs(coefs3 - true.betas)),
  negBLASSO = sum(abs(coefs4 - true.betas)),
  BayesElasticNet = sum(abs(coefs5 - true.betas))
)

EstimationError
```


<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;margin:0px auto;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-dup3{font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;border-color:#000000;text-align:left;vertical-align:top}
.tg .tg-t6vj{font-weight:bold;font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;color:#009901;border-color:#000000;text-align:center;vertical-align:top}
.tg .tg-pz14{font-weight:bold;font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;border-color:#000000;text-align:left;vertical-align:top}
.tg .tg-9j9p{font-weight:bold;font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;color:#cb0000;border-color:#000000;text-align:center;vertical-align:top}
.tg .tg-nb89{font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;border-color:#000000;text-align:center;vertical-align:top}
.tg .tg-d7ip{font-weight:bold;font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;border-color:#000000;text-align:left}
.tg .tg-04du{font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;border-color:#000000;text-align:center}
.tg .tg-wqug{font-weight:bold;font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;color:#009901;border-color:#000000;text-align:center}
.tg .tg-2i1y{font-weight:bold;font-family:"Palatino Linotype", "Book Antiqua", Palatino, serif !important;;color:#cb0000;border-color:#000000;text-align:center}
</style>
<table class="tg">
  <tr>
    <th class="tg-d7ip">trueBetas      </th>
    <th class="tg-d7ip">OLS</th>
    <th class="tg-pz14">BayesGLM</th>
    <th class="tg-pz14">normalBernoulliMixture</th>
    <th class="tg-pz14">extendedBLASSO</th>
    <th class="tg-pz14">negBLASSO</th>
    <th class="tg-pz14">BayesElasticNet</th>
  </tr>
  <tr>
    <td class="tg-04du">1</td>
    <td class="tg-wqug">1</td>
    <td class="tg-t6vj">1</td>
    <td class="tg-t6vj">1</td>
    <td class="tg-t6vj">1</td>
    <td class="tg-t6vj">1</td>
    <td class="tg-t6vj">1</td>
  </tr>
  <tr>
    <td class="tg-04du">0</td>
    <td class="tg-2i1y">-1</td>
    <td class="tg-9j9p">-1</td>
    <td class="tg-9j9p">-1</td>
    <td class="tg-9j9p">-1</td>
    <td class="tg-9j9p">-1</td>
    <td class="tg-9j9p">-1</td>
  </tr>
  <tr>
    <td class="tg-04du">1</td>
    <td class="tg-wqug">1</td>
    <td class="tg-t6vj">1</td>
    <td class="tg-t6vj">1</td>
    <td class="tg-t6vj">1</td>
    <td class="tg-t6vj">1</td>
    <td class="tg-t6vj">1</td>
  </tr>
  <tr>
    <td class="tg-04du">0</td>
    <td class="tg-2i1y">1</td>
    <td class="tg-9j9p">1</td>
    <td class="tg-9j9p">1</td>
    <td class="tg-9j9p">1</td>
    <td class="tg-9j9p">1</td>
    <td class="tg-9j9p">1</td>
  </tr>
  <tr>
    <td class="tg-nb89">0</td>
    <td class="tg-9j9p">1</td>
    <td class="tg-9j9p">1</td>
    <td class="tg-9j9p">1</td>
    <td class="tg-9j9p">1</td>
    <td class="tg-9j9p">1</td>
    <td class="tg-9j9p">1</td>
  </tr>
  <tr>
    <td class="tg-nb89">-1</td>
    <td class="tg-t6vj">-1</td>
    <td class="tg-t6vj">-1</td>
    <td class="tg-t6vj">-1</td>
    <td class="tg-t6vj">-1</td>
    <td class="tg-t6vj">-1</td>
    <td class="tg-t6vj">-1</td>
  </tr>
  <tr>
    <td class="tg-nb89">-1</td>
    <td class="tg-9j9p">1</td>
    <td class="tg-t6vj">-1</td>
    <td class="tg-t6vj">-1</td>
    <td class="tg-t6vj">-1</td>
    <td class="tg-t6vj">-1</td>
    <td class="tg-t6vj">-1</td>
  </tr>
  <tr>
    <td class="tg-nb89">1</td>
    <td class="tg-9j9p">-1</td>
    <td class="tg-t6vj">1</td>
    <td class="tg-t6vj">1</td>
    <td class="tg-t6vj">1</td>
    <td class="tg-t6vj">1</td>
    <td class="tg-t6vj">1</td>
  </tr>
  <tr>
    <td class="tg-nb89">1</td>
    <td class="tg-9j9p">-1</td>
    <td class="tg-t6vj">1</td>
    <td class="tg-t6vj">1</td>
    <td class="tg-9j9p">-1</td>
    <td class="tg-9j9p">-1</td>
    <td class="tg-t6vj">1</td>
  </tr>
  <tr>
    <td class="tg-dup3">Number of Incorrect Signs<br>(Sum of Absolute Estimation Errors)</td>
    <td class="tg-nb89">6<br>(12.714)</td>
    <td class="tg-nb89">3<br>(2.683)</td>
    <td class="tg-nb89">3<br>(2.059)</td>
    <td class="tg-nb89">4<br>(2.417)</td>
    <td class="tg-nb89">4<br>(2.462)</td>
    <td class="tg-nb89">3<br>(2.663)</td>
  </tr>
</table>