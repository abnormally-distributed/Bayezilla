---
title: "R Notebook"
output: 
  html_notebook: 
    code_folding: hide
---

Load the package..

```{r include=FALSE}
library(Bayezilla)
```

```{r include=FALSE}
# Simulate data
library(fBasics)
library(gdata)

set.seed(101)  # for reproducibility
N <- 65
P <- 15

#True betas
betas = c(.9927551, 0, .924, -1.18113776, .456, 0.394, -1.05, .89, 0, 1.04, 0, -0.47034, 1.01, -.73, 0) 
Intercept <- 0

#Generate Covariance Matrix
#Generate data simulation
Sigma = rcorr(P, 1)

set.seed(104)
# Generate data
X = MASS::mvrnorm(N, mu = rep(0, P), Sigma = Sigma, empirical = TRUE)
X = as.matrix(scale(X))
y = as.vector(X %*% matrix(betas, ncol = 1))

# Add gaussian noise
X = X + matrix(rnorm((N * P)/4 , .25, 1.125), N, P)
X = scale(X)
colnames(X) = paste0("X", 1:P)
data = cbind.data.frame(Y = y, X)
```

```{r include=FALSE}
readr::write_csv(data, "simulation_data1.csv")
```

NOTE: 90% Confidence Intervals are used throughout.

## Ordinary Least Squares

```{r echo=FALSE, paged.print=FALSE}
broom::tidy(lm(Y ~ ., data), conf.int =TRUE, conf.level = .90) %>% mutate_if(is.numeric, round, 3)
lmcoefs = broom::tidy(lm(Y ~ ., data)) %>% mutate_if(is.numeric, round, 3)
lmcoefs = lmcoefs$estimate[-1]

AICc = MuMIn::AICc(lm(Y~.,data))
```

## Normal-Gamma(.5, .5) Mixture Prior 

NOTE: The Gamma(.5, .5) prior serves as a mixing parameter on the precision of independent normal priors on the coefficients. Integrating out this mixing parameter implies marginal  independent Cauchy distribution priors (student-t with one degree of freedom) with scale 1. 

```{r include=FALSE, paged.print=FALSE}
out = glmBayes(Y ~ ., data, log_lik = TRUE, cl = makeCluster(4))
```

```{r echo=FALSE, paged.print=FALSE}
post_summary(out)[1:19,]
coefs1 = post_summary(out)$estimate[2:16]
```

## Extended Bayesian LASSO

```{r include=FALSE, paged.print=FALSE}
out3 = extLASSO(Y ~ ., data, cl = makeCluster(4), log_lik = TRUE)
```

```{r echo=FALSE, paged.print=FALSE}
post_summary(out3)[1:19,]
coefs3 = post_summary(out3)$median[2:16]
```

## Normal-Exponential-Gamma (NEG) Bayesian LASSO

```{r include=FALSE, paged.print=FALSE}
out4 = negLASSO(Y ~ ., data, cl = makeCluster(4), log_lik = TRUE)
```

```{r echo=FALSE, paged.print=FALSE}
post_summary(out4)[c(1:19,27),]
coefs4 = post_summary(out4)$median[2:16]
```

## Bayesian Elastic Net

```{r include=FALSE, paged.print=FALSE}
out5 = bayesEnet(Y ~ ., data, cl = makeCluster(4), log_lik = TRUE)
```

```{r echo=FALSE, paged.print=FALSE}
post_summary(out5)[1:19,]
coefs5 = post_summary(out5)$median[2:16]
```


## Evaluating the Models

### Who estimated the true effects most accurately?

```{r echo=FALSE, paged.print=FALSE}
true.betas = round(c(.9927551, 0, .924, -1.18113776, .456, 0.394, -1.05, .89, 0, 1.04, 0, -0.47034, 1.01, -.73, 0), 3)
coefTable = data.frame(
trueBetas = true.betas,
OLS = lmcoefs,
BayesGLM = coefs1,
extendedBLASSO = coefs3,
negBLASSO = coefs4,
BayesElasticNet = coefs5
)

EstimationError = cbind.data.frame(
  trueBetas = "Sum of Abs. Errs.",
  OLS = sum(abs(lmcoefs[-1] - true.betas)),
  BayesGLM = sum(abs(coefs1 - true.betas)),
  extendedBLASSO = sum(abs(coefs3 - true.betas)),
  negBLASSO = sum(abs(coefs4 - true.betas)),
  BayesElasticNet = sum(abs(coefs5 - true.betas))
)

AvgErr = cbind.data.frame(
  trueBetas = "Avg err. per beta",
  OLS = sum(abs(lmcoefs[-1] - true.betas))/P,
  BayesGLM = sum(abs(coefs1 - true.betas))/P,
  extendedBLASSO = sum(abs(coefs3 - true.betas))/P,
  negBLASSO = sum(abs(coefs4 - true.betas))/P,
  BayesElasticNet = sum(abs(coefs5 - true.betas))/P
)

rbind(coefTable, EstimationError, AvgErr)
```


### Who most accurately fit the values of the dependent variable?



### Who is anticipated to have the best out of sample predictive performance?


