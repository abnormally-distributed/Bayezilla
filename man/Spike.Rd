% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/glm_spike.R
\name{Spike}
\alias{Spike}
\title{Bernoulli-Normal Mixture Selection for GLMs}
\usage{
Spike(formula, data, family = "gaussian", phi_prior = c(0.5, 0.5),
  log_lik = FALSE, iter = 10000, warmup = 1000, adapt = 2000,
  chains = 4, thin = 3, method = "parallel", cl = makeCluster(2),
  ...)
}
\arguments{
\item{formula}{the model formula}

\item{data}{a data frame}

\item{family}{one of "gaussian", "binomial", or "poisson".}

\item{phi_prior}{The beta distribution parameters on the inclusion probabilities. Default is Jeffrey's prior, c(0.5, 0.5).}

\item{log_lik}{Should the log likelihood be monitored? The default is FALSE.}

\item{iter}{How many post-warmup samples? Defaults to 10000.}

\item{warmup}{How many warmup samples? Defaults to 1000.}

\item{adapt}{How many adaptation steps? Defaults to 2000.}

\item{chains}{How many chains? Defaults to 4.}

\item{thin}{Thinning interval. Defaults to 3.}

\item{method}{Defaults to "parallel". For an alternative parallel option, choose "rjparallel" or. Otherwise, "rjags" (single core run).}

\item{cl}{Use parallel::makeCluster(# clusters) to specify clusters for the parallel methods. Defaults to two cores.}

\item{...}{Other arguments to run.jags.}
}
\value{
A run.jags object
}
\description{
This is the most basic type of Bayesian variable selection. This models the
regression coefficients as coming from either a null distribution represented
by a probability mass of 100% at zero, or as coming from a normal distribution. A student-t family can be obtained
as a norma-gamma mixture by utilizing a gamma(nu/2, nu/2) distribution where nu is the desired degrees of freedom.
One degree of freedom yields gamma(.5, .5), which is the cauchy distribution. Hence, this model results in 
marginal independent cauchy priors on each coefficient. The cauchy distribution has no defined first or second moments
(mean and variance) and hence is an ideal proper reference prior. The cauchy distribution's extremely
long tails allow coefficients with strong evidence of being large to not be shrunk too strongly, while the large
probability mass at the mode of zero captures small noisy coefficients and regularizes them. This adaptive shrinkage
property results in an ideal prior.

Note that you should center and scale your predictor variables before using this function.
If you do not scale and center your numeric predictors, this will likely not perform well or
give reasonable results. The mixing hyperparameter omega assumes all covariates are on the same scale.

This model works best on smaller to medium sized data sets with a small number of variables (less than 20). 
If you experience difficulty with running times or obtaining a good effective sample size consider using the extended LASSO. 
Another tip that may improve performance is using a beta(1, 1) or beta(1.5, 1.5) prior on phi. If all coefficients are set to
zero and there is truly good reason to believe this is not correct (ie, you aren't just hunting for "statistical
significance", are you?) a prior such as beta(4, 2) may help. If there is no sparsity, and you have genuine reason
to believe there should be try a beta(2,8) prior (if not, ask yourself why are you using variable selection?
If it is to deal with collinearity or other foibles for least squares, consider trying the glm_bayes function instead).
}
\examples{
glmSpike()

}
