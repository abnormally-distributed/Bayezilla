% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/apcGlm.R
\name{apcGlm}
\alias{apcGlm}
\title{Adaptive Powered Correlation Prior}
\usage{
apcGlm(formula, data, family = "gaussian", lower = -10, upper = 10,
  log_lik = FALSE, iter = 10000, warmup = 2500, adapt = 7500,
  chains = 4, thin = 1, method = "parallel", cl = makeCluster(2),
  ...)
}
\arguments{
\item{formula}{the model formula}

\item{data}{a data frame}

\item{family}{one of "gaussian", "binomial", or "poisson".}

\item{lower}{lower limit on value of lambda. Defaults to -10. If the model is failing due to a non-invertible
matrix, try adjusting this number.}

\item{log_lik}{Should the log likelihood be monitored? The default is FALSE.}

\item{iter}{How many post-warmup samples? Defaults to 10000.}

\item{warmup}{How many warmup samples? Defaults to 1000.}

\item{adapt}{How many adaptation steps? Defaults to 2000.}

\item{chains}{How many chains? Defaults to 4.}

\item{thin}{Thinning interval. Defaults to 1.}

\item{method}{Defaults to "parallel". For an alternative parallel option, choose "rjparallel". Otherwise, "rjags" (single core run).}

\item{cl}{Use parallel::makeCluster(# clusters) to specify clusters for the parallel methods. Defaults to two cores.}

\item{...}{Other arguments to run.jags.}

\item{uppper}{upper limit on value of lambda. Defaults to 10. If the model is failing due to a non-invertible
matrix, try adjusting this number.}
}
\value{
A run.jags object.
}
\description{
This function implements the adaptive powered correlation prior for estimating a single general(ized) linear regression
model. 
\cr
The adaptive powered correlation prior extends the Zellner-Siow Cauchy g-prior by allowing the crossproduct of the 
model matrix to be raised to powers other than -1 (which gives the Fisher information matrix). The power here will 
be referred to as "lambda". A lambda of 0 results in an identity matrix, which results in a ridge-regression like
prior. Positive values of lambda adapt to collinearity by allowing correlated predictors to enter and exit the model 
together. Negative values of lambda on the other hand favor including only one of a set of correlated predictors. 
This can be understood as projecting the information matrix into a new space which leads to a model
similar in function to principal components regression (Krishna et al., 2009). In this implementation full Bayesian
inference is used for lambda, rather than searching via marginal likelihood maximization as Krishna et al. (2009) did. 
The reason for this is twofold. First, full Bayesian inference means the model has to be fit only once instead of
several times over a grid of candidate values for lambda. Second, this avoids any coherency problems such as those
that arise when using fixed-g priors.
\cr
\cr
The model specification is given below. Note that the model formulae have been adjusted to reflect the fact that JAGS
parameterizes the normal and multivariate normal distributions by their precision, rater than (co)variance. 
For generalized linear models plug-in pseudovariances are used. 
\cr
\cr
\if{html}{\figure{apc.png}{}}
\if{latex}{\figure{apc.png}{}}
\cr
\cr
Plugin Pseudo-Variances: \cr
\if{html}{\figure{pseudovar.png}{}}
\if{latex}{\figure{pseudovar.png}{}}
}
\examples{
apcGlm()

}
\references{
Zellner, A. & Siow S. (1980). Posterior odds ratio for selected regression hypotheses. In Bayesian statistics. Proc. 1st int. meeting (eds J. M. Bernardo, M. H. DeGroot, D. V. Lindley & A. F. M. Smith), 585–603. University Press, Valencia. \cr 
\cr
Liang, Paulo, Molina, Clyde, & Berger (2008) Mixtures of g Priors for Bayesian Variable Selection, Journal of the American Statistical Association, 103:481, 410-423, DOI: 10.1198/016214507000001337 \cr
\cr
Krishna, A., Bondell, H. D., & Ghosh, S. K. (2009). Bayesian variable selection using an adaptive powered correlation prior. Journal of statistical planning and inference, 139(8), 2665–2674. doi:10.1016/j.jspi.2008.12.004
\cr
}
