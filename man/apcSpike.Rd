% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/apcSpike.R, R/apcSpikeDC.R
\name{apcSpike}
\alias{apcSpike}
\title{Bernoulli-Normal Adaptive Powered Correlation Prior for Variable Selection}
\usage{
apcSpike(formula, design.formula, data, family = "gaussian",
  lambda = -1, log_lik = FALSE, iter = 10000, warmup = 1000,
  adapt = 5000, chains = 4, thin = 1, method = "rjparallel",
  cl = makeCluster(2), ...)

apcSpike(formula, design.formula, data, family = "gaussian",
  lambda = -1, log_lik = FALSE, iter = 10000, warmup = 1000,
  adapt = 5000, chains = 4, thin = 1, method = "rjparallel",
  cl = makeCluster(2), ...)
}
\arguments{
\item{formula}{the model formula}

\item{design.formula}{formula for the design covariates.}

\item{data}{a data frame}

\item{family}{one of "gaussian", "binomial", or "poisson".}

\item{lambda}{the power to use in the adaptive correlation prior. Default is -1, which gives the Zellner-Siow g prior. Setting
lambda to 0 results in a ridge-regression like prior. Setting lambda to a positive value adapts to collinearity by shrinking 
collinear variables towards a common value. Negative values of lambda pushes collinear variables further apart. I suggest 
fitting multiple values of lambda and selecting which is best via LOO-IC or WAIC.}

\item{log_lik}{Should the log likelihood be monitored? The default is FALSE.}

\item{iter}{How many post-warmup samples? Defaults to 10000.}

\item{warmup}{How many warmup samples? Defaults to 1000.}

\item{adapt}{How many adaptation steps? Defaults to 2000.}

\item{chains}{How many chains? Defaults to 4.}

\item{thin}{Thinning interval. Defaults to 1.}

\item{method}{Defaults to "rjparallel". For an alternative parallel option, choose "parallel". Otherwise, "rjags" (single core run).}

\item{cl}{Use parallel::makeCluster(# clusters) to specify clusters for the parallel methods. Defaults to two cores.}

\item{...}{Other arguments to run.jags.}

\item{formula}{the model formula}

\item{data}{a data frame}

\item{lambda}{the power to use in the adaptive correlation prior. Default is -1, which gives the Zellner-Siow g prior. Setting
lambda to 0 results in a ridge-regression like prior. Setting lambda to a positive value adapts to collinearity by shrinking 
collinear variables towards a common value. Negative values of lambda pushes collinear variables further apart. I suggest 
fitting multiple values of lambda and selecting which is best via LOO-IC or WAIC.}

\item{family}{one of "gaussian", "binomial", or "poisson".}

\item{log_lik}{Should the log likelihood be monitored? The default is FALSE.}

\item{iter}{How many post-warmup samples? Defaults to 10000.}

\item{warmup}{How many warmup samples? Defaults to 1000.}

\item{adapt}{How many adaptation steps? Defaults to 2000.}

\item{chains}{How many chains? Defaults to 4.}

\item{thin}{Thinning interval. Defaults to 1.}

\item{method}{Defaults to "rjparallel". For an alternative parallel option, choose "parallel". Otherwise, "rjags" (single core run).}

\item{cl}{Use parallel::makeCluster(# clusters) to specify clusters for the parallel methods. Defaults to two cores.}

\item{...}{Other arguments to run.jags.}
}
\value{
A run.jags object.

A run.jags object.
}
\description{
IMPORTANT: I suggest not using any factor predictor variables, only numeric. In my experience the inclusion of categorical
predictors tends to lead to odd results in calculating the prior scale. \cr
\cr
This function impements the adaptive powered correlation g-prior model described by Krishna, Bondell, and Ghosh (2009). Typically, in regression the cross-product XtX is inverted in the process of calculating the coefficients. In addition, 
the Zellner-Siow cauchy g-prior utilizes the inverse crossproduct is used as an empirical Bayesian method of determining the proper scale of the coefficient
priors by treating the inverse crossproduct as a covariance matrix, which is scaled by the parameter "g". \cr
\cr
The adaptive powered correlation prior simply extends this to allow using other powers besides -1. The power here will be referred to as "lambda".
Setting lambda to 0 results in a ridge-regression like prior. Setting lambda to a positive value adapts to collinearity by allowing
correlated predictors to enter and exit the model together. Negative values of lambda on the other hand favor including only one
of a set of correlated predictors. Of course, setting lambda to -1 is just the Zellner-Siow cauchy g-prior. This is designed to deal with collinearity in a more adaptive way than even ridge regression
by allowing the analyst to use model comparison techniques to choose an optimal value of lambda, and then using the best model for inference.
An analysts beliefs about which type of selection is preferred, or the goals of the particular analysis, can also inform the choice of lambda.
\cr
\cr
The probability that a coefficient comes from the null-spike is controlled by a hyperparameter "phi" which estimates the overall probability of inclusion, i.e., the proportion of the P-number of predictors that are non-zero. 
This hyperparameter is given a Jeffrey's prior, beta(1/2, 1/2) which is non-informative and objective.
\cr
\cr
Note, however, that this prior is designed to deal with collinearity but not necessarily P > N scenarios. For that you may wish to take a look
at the \code{\link[Bayezilla]{extLASSO}} or \code{\link[Bayezilla]{bayesEnet}} functions. 
\cr

IMPORTANT: I suggest not using any factor predictor variables, only numeric. In my experience the inclusion of categorical
predictors tends to lead to odd results in calculating the prior scale. \cr
\cr
This function impements the adaptive powered correlation g-prior model described by 
Krishna, Bondell, and Ghosh (2009), but with the allowance for a set of covariates that
are not penalized. For example, you may wish to include variables such
as age and gender in all models so that the coefficients for the other variables are penalized while
controlling for these. This is a common need in research. \cr 
\cr
\cr
Typically, in regression the cross-product XtX is inverted in the process of calculating the coefficients. In addition, 
the Zellner-Siow cauchy g-prior utilizes the inverse crossproduct is used as an empirical Bayesian method of determining the proper scale of the coefficient
priors by treating the inverse crossproduct as a covariance matrix, which is scaled by the parameter "g". \cr
\cr
The adaptive powered correlation prior simply extends this to allow using other powers besides -1. The power here will be referred to as "lambda".
Setting lambda to 0 results in a ridge-regression like prior. Setting lambda to a positive value adapts to collinearity by allowing
correlated predictors to enter and exit the model together. Negative values of lambda on the other hand favor including only one
of a set of correlated predictors. Of course, setting lambda to -1 is just the Zellner-Siow cauchy g-prior. This is designed to deal with collinearity in a more adaptive way than even ridge regression
by allowing the analyst to use model comparison techniques to choose an optimal value of lambda, and then using the best model for inference.
An analysts beliefs about which type of selection is preferred, or the goals of the particular analysis, can also inform the choice of lambda.
\cr
\cr
The probability that a coefficient comes from the null-spike is controlled by a hyperparameter "phi" which estimates the overall probability of inclusion, i.e., the proportion of the P-number of predictors that are non-zero. 
This hyperparameter is given a Jeffrey's prior, beta(1/2, 1/2) which is non-informative and objective.
\cr
}
\examples{
apcSpike()

apcSpikeDC()

}
\references{
Krishna, A., Bondell, H. D., & Ghosh, S. K. (2009). Bayesian variable selection using an adaptive powered correlation prior. Journal of statistical planning and inference, 139(8), 2665–2674. doi:10.1016/j.jspi.2008.12.004 \cr
\cr
Kuo, L., & Mallick, B. (1998). Variable Selection for Regression Models. Sankhyā: The Indian Journal of Statistics, Series B, 60(1), 65-81. \cr

Krishna, A., Bondell, H. D., & Ghosh, S. K. (2009). Bayesian variable selection using an adaptive powered correlation prior. Journal of statistical planning and inference, 139(8), 2665–2674. doi:10.1016/j.jspi.2008.12.004 \cr
\cr
Kuo, L., & Mallick, B. (1998). Variable Selection for Regression Models. Sankhyā: The Indian Journal of Statistics, Series B, 60(1), 65-81. \cr
}
