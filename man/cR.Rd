% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cR.R
\name{cR}
\alias{cR}
\title{Conditional Bias Corrected Re-Estimation of Selected Variables}
\usage{
cR(formula, data, delta = NULL, family = "gaussian", log_lik = FALSE,
  iter = 10000, warmup = 1000, adapt = 1000, chains = 4,
  thin = 1, method = "parallel", cl = makeCluster(2), ...)
}
\arguments{
\item{formula}{the model formula}

\item{data}{a data frame}

\item{delta}{a vector of inclusion indicators. Must only contain values of 0 and 1.}

\item{family}{one of "gaussian", "binomial", or "poisson".}

\item{log_lik}{Should the log likelihood be monitored? The default is FALSE.}

\item{iter}{How many post-warmup samples? Defaults to 10000.}

\item{warmup}{How many warmup samples? Defaults to 1000.}

\item{adapt}{How many adaptation steps? Defaults to 1000.}

\item{chains}{How many chains? Defaults to 4.}

\item{thin}{Thinning interval. Defaults to 1.}

\item{method}{Defaults to "parallel". For an alternative parallel option, choose "rjparallel" or. Otherwise, "rjags" (single core run).}

\item{cl}{Use parallel::makeCluster(# clusters) to specify clusters for the parallel methods. Defaults to two cores.}

\item{...}{Other arguments to run.jags.}
}
\value{
A run.jags object
}
\description{
At times the bias induced by continuous shrinkage prior methods such as the Bayesian LASSO may be too great to be
ignored and gains in predictive and inferential accuracy can be made by performing conditional re-estimation.  The simplest 
form of this is the OLS-LASSO / MLE-LASSO hybrid of Efron et al (2004), where the LASSO-selected variables are re-estimated 
using maximum likelihood. However, this ignores selection effects. Alternatives are the Relaxed LASSO procedure of Meinshausen N (2007)
which performs a second LASSO on the variables selected by the first LASSO. The second pass of the LASSO will select a smaller 
penalty due to the reduced variable set, hence providing estimates closer to the MLEs while retaining some small shrinkage to attenuate
optimistic selection effects. A general frequentist re-estimation procedure has been put forth by Taylor & Tibsharini (2015) that
is general enough to be used with various variable selection methods. Similarly, Sillanpää &  Mutshinda (2011) provided a means
of conditional re-estimation in their discussion of post-selection inference using the shape adaptive shrinkage prior (offered here
in the \code{\link[Bayezilla]{sasp}} function). The benefits of 
such two-stage estimation procedures are discussed by Belloni & Chernozhukov (2013). Belloni & Chernozhukov demonstrate that such
re-estimation dominates the naive MLE-LASSO (or MLE-your-favorite-variable-selection-method-here) procedure. \cr

\cr
The method of Sillanpää &  Mutshinda is adapted here. Their conditional re-estimation procedure involves supplying the full model
matrix that was passed to the LASSO or other selection procedure, along with a vector of indicator variables that take values of [0, 1]
to indicate the elimination or inclusion of the variable. They then estimate the raw coefficients using vague normal priors and multiply
these by the indicator variables. Here cauchy priors are used. This is done to account for the eliminated variables for purposes of avoiding selection bias, while
also only obtaining non-zero estimates for the selected variables.  

\cr

This procedure can be used following any variable selection process, Bayesian or not. All you need to do is provide the full model
formula and a vector of inclusion indicators to utilize this. This function assumes the variables are 
standardized just as the selection models require. \cr 

Standard gaussian, binomial, and poisson likelihood functions are available. \cr
\cr
Model Specification:\cr
\cr
\if{html}{\figure{cR.png}{}}
\if{latex}{\figure{cR.png}{}}
}
\examples{
cR(formula, data, delta = c(0, 0, 1, 1, 1, 0, 1, 0))
}
\references{
Belloni, A.; Chernozhukov, V. (2013) Least squares after model selection in high-dimensional sparse models. Bernoulli 19, no. 2, 521--547. doi:10.3150/11-BEJ410.  \cr
\cr
Efron B., Hastie T., Johnstone I., Tibshirani, R. (2004). Least angle regression. Ann Stat 32: 407–499. \cr
\cr
Meinshausen N (2007). Relaxed Lasso. Comp Stat Data Anal 52: 374–393. \cr
\cr
Sillanpää, S., &  Mutshinda, C., (2011) Bayesian shrinkage analysis of QTLs under shape-adaptive shrinkage priors, and accurate re-estimation of genetic effects. Heredity volume 107, pages 405–412. doi: 10.1038/hdy.2011.37 \cr
\cr
Taylor, J., & Tibshirani, R. (2015) Statistical learning and selective inference. Proceedings of the National Academy of Sciences, 112 (25) 7629-7634; doi: 10.1073/pnas.1507583112 \cr
\cr
}
