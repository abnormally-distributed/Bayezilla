% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cR.R
\name{cR}
\alias{cR}
\title{Conditional Bias Corrected Re-Estimation of Selected Variables}
\usage{
cR(formula, data, delta = NULL, family = "gaussian", log_lik = FALSE,
  iter = 10000, warmup = 1000, adapt = 1000, chains = 4,
  thin = 1, method = "parallel", cl = makeCluster(2), ...)
}
\arguments{
\item{formula}{the model formula}

\item{data}{a data frame}

\item{delta}{a vector of inclusion indicators. Must only contain values of 0 and 1.}

\item{family}{one of "gaussian", "binomial", or "poisson".}

\item{log_lik}{Should the log likelihood be monitored? The default is FALSE.}

\item{iter}{How many post-warmup samples? Defaults to 10000.}

\item{warmup}{How many warmup samples? Defaults to 1000.}

\item{adapt}{How many adaptation steps? Defaults to 1000.}

\item{chains}{How many chains? Defaults to 4.}

\item{thin}{Thinning interval. Defaults to 1.}

\item{method}{Defaults to "parallel". For an alternative parallel option, choose "rjparallel" or. Otherwise, "rjags" (single core run).}

\item{cl}{Use parallel::makeCluster(# clusters) to specify clusters for the parallel methods. Defaults to two cores.}

\item{...}{Other arguments to run.jags.}
}
\value{
A run.jags object
}
\description{
Some argue that LASSO-type variable selections should have the selected variables re-estimated with unpenalized estimators.  
The simplest form of this is the LASSO-OLS two step procedure of Efron et al (2004), where the LASSO-selected variables are re-estimated using maximum likelihood.  
Similarly, Sillanpää &  Mutshinda (2011) provided a means
of conditional re-estimation in their discussion of post-selection inference using the shape adaptive shrinkage prior (offered here in the \code{\link[Bayezilla]{sasp}}
function). The benefits of such two-stage estimation procedures are discussed by Belloni & Chernozhukov (2013). \cr

\cr
The method of Sillanpää &  Mutshinda is adapted here. Their conditional re-estimation procedure involves supplying the full model 
matrix that was passed to the LASSO or other selection procedure, along with a vector of indicator variables that take values of [0, 1] to indicate 
the elimination or inclusion of the variable. They then estimate the raw coefficients using vague normal priors and multiply these by the indicator variables. 

\cr This procedure can be used following any variable selection process, Bayesian or not. All you need to do is provide the full model formula and a vector of 
inclusion indicators to utilize this. This function assumes the variables are 
standardized just as the selection models require. \cr Personally I am not sure how I feel about this type of procedure. 
From a Bayesian perspective, the LASSO (or other similar model) estimates are the best parameter evidence, conditioned on the model and data. 
To re-estimate the model seems an awful lot like using the data twice. In the frequentist paradigm the confidence
intervals on the re-estimated model are adjusted, as in Taylor & Tibshirani (2015) to account for selection effects but I do
not see how this would be straightforward or justified in a Bayesian paradigm. \cr
\cr

Standard gaussian, binomial, and poisson likelihood functions are available. \cr
\cr
Model Specification:\cr
\cr
\if{html}{\figure{cR.png}{}}
\if{latex}{\figure{cR.png}{}}
}
\examples{
cR(formula, data, delta = c(0, 0, 1, 1, 1, 0, 1, 0))
}
\references{
Belloni, A.; Chernozhukov, V. (2013) Least squares after model selection in high-dimensional sparse models. Bernoulli 19, no. 2, 521--547. doi:10.3150/11-BEJ410.  \cr
\cr
Efron B., Hastie T., Johnstone I., Tibshirani, R. (2004). Least angle regression. Ann Stat 32: 407–499. \cr
\cr
Meinshausen N (2007). Relaxed Lasso. Comp Stat Data Anal 52: 374–393. \cr
\cr
Sillanpää, S., &  Mutshinda, C., (2011) Bayesian shrinkage analysis of QTLs under shape-adaptive shrinkage priors, and accurate re-estimation of genetic effects. Heredity volume 107, pages 405–412. doi: 10.1038/hdy.2011.37 \cr
\cr
Taylor, J., & Tibshirani, R. (2015) Statistical learning and selective inference. Proceedings of the National Academy of Sciences, 112 (25) 7629-7634; doi: 10.1073/pnas.1507583112 \cr
\cr
}
