% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/glm_bayes.R
\name{glmBayes}
\alias{glmBayes}
\title{Bayesian Basic GLMs}
\usage{
glmBayes(formula, data, family = "gaussian", log_lik = FALSE,
  iter = 10000, warmup = 1000, adapt = 2000, chains = 4,
  thin = 3, method = "rjags", cl = NULL, ...)
}
\arguments{
\item{formula}{the model formula}

\item{data}{a data frame}

\item{family}{one of "gaussian", "binomial", or "poisson".}

\item{log_lik}{Should the log likelihood be monitored? The default is FALSE.}

\item{iter}{How many post-warmup samples? Defaults to 10000.}

\item{warmup}{How many warmup samples? Defaults to 1000.}

\item{adapt}{How many adaptation steps? Defaults to 2000.}

\item{chains}{How many chains? Defaults to 4.}

\item{thin}{Thinning interval. Defaults to 3.}

\item{method}{Defaults to "rjags" (single core run). For parallel, choose "rjparallel" or "parallel".}

\item{cl}{Use parallel::makeCluster(# clusters) to specify clusters for the parallel methods.}

\item{...}{Other arguments to run.jags.}
}
\value{
A run.jags object
}
\description{
This function utilizes a model where the prior precision on the coefficients is learned from the data
through a hyper-prior. This hyperparameter "omega" allows the model to flexibly adapt to the data. In particular
this hyperprior is a gamma(.5, .5) distribution which results in independent marginal cauchy distributions. The
very heavy tails of the cauchy allow for maximum sensitivity to large coefficients, while the dense spike at zero
allows the model to shrink the coefficients if neccesary. This process is completely data driven. Standard gaussian,
binomial, and poisson likelihood functions are available. Note that if you do not scale and center your numeric predictors, this will likely not perform well or
give reasonable results. The mixing hyperparameter omega assumes all covariates are on the same scale.
}
\examples{
glmBayes()

}
