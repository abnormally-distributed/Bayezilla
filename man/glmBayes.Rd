% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/glmBayes.R
\name{glmBayes}
\alias{glmBayes}
\title{Bayesian GLMs}
\usage{
glmBayes(formula, data, family = "gaussian", s = 1, df = 1,
  log_lik = FALSE, iter = 10000, warmup = 1000, adapt = 2000,
  chains = 4, thin = 1, method = "parallel", cl = makeCluster(2),
  ...)
}
\arguments{
\item{formula}{the model formula}

\item{data}{a data frame}

\item{family}{one of "gaussian", "binomial", or "poisson".}

\item{s}{The desired prior scale. Defaults to 1. Is automatically squared within the model so
select a number here on the standard deviation scale.}

\item{log_lik}{Should the log likelihood be monitored? The default is FALSE.}

\item{iter}{How many post-warmup samples? Defaults to 10000.}

\item{warmup}{How many warmup samples? Defaults to 1000.}

\item{adapt}{How many adaptation steps? Defaults to 2000.}

\item{chains}{How many chains? Defaults to 4.}

\item{thin}{Thinning interval. Defaults to 1.}

\item{method}{Defaults to "parallel". For an alternative parallel option, choose "rjparallel" or. Otherwise, "rjags" (single core run).}

\item{cl}{Use parallel::makeCluster(# clusters) to specify clusters for the parallel methods. Defaults to two cores.}

\item{...}{Other arguments to run.jags.}
}
\value{
A run.jags object
}
\description{
This model utilizes Student-t priors for the coefficients. It is parameterized
as a scale mixture of Gaussians, which works by setting a gamma prior on the precision of the 
normal with shape and rate parameters equal to the desired degrees of freedom divided by two (the
shape parameter can also be multiplied by a variance to give the resulting student-t distribution
the appropriately scaled precision) :  
\if{html}{\figure{dof.png}{}}
\if{latex}{\figure{dof.png}{}}
\cr \cr
By default, the Cauchy distribution is obtained by setting the degrees of freedom to 1. The default 
prior standard deviation is also 1. While JAGS has a Student-t distribution built in, 
the samplers in JAGS tend to sample from Student-t 
distributions very inefficiently at times. By parameterizing the Student-t 
distribution this way JAGS can take advantage of the normal-gamma conjugacy with 
Gibbs sampling and sample very quickly and accurately. \cr
\cr
The default prior settings assume your data are standardized to mean zero and standard deviation of 1. 
\cr \cr
For an alternative prior that also performs very well see \code{\link[Bayezilla]{apcGlm}}, which when
lambda = -1 gives the Zellner-Siow Cauchy g-prior.
\cr
The full model structure is given below: \cr
\cr
\cr
\if{html}{\figure{glm.png}{}}
\if{latex}{\figure{glm.png}{}}
\cr
\cr
The relationship between the degrees of freedom and the prior precision 
(inverse of squared prior standard deviation)
is represented in the figure below. Essentially, if the precision 
is sufficiently small (or equivalently, if prior s.d. is large) the amount of shrinkage
is essentially none regardless of the degrees of freedom. Increasing the degrees of freedom
also leads to greater regularization. However, the pattern of regularization differs for different
values, which is not represented in the figure. Supposing the precision is held constant at 1, 
degrees of freedom  < 8 will tend to shrink smaller 
coefficients to a greater degree, while larger coefficients will be affected less due to the long tails. However, 
as the degrees of freedom increase the Student-t distribution will take on an increasingly gaussian shape and the
tails will be pulled in, giving more uniform shrinkage (at this point it is effectively a ridge regression). If the
precision is increased, the contrast between small and large coefficients will tend to be even greater for small
degrees of freedom, while higher degrees of freedom approach a highly regularized ridge solution. \cr
\cr
\if{html}{\figure{shrinkagelowres.png}{}}
\if{latex}{\figure{shrinkagelowres.png}{}}
}
\examples{
glmBayes()


}
