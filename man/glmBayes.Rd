% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/glmBayes.R
\name{glmBayes}
\alias{glmBayes}
\title{Bayesian Regularized GLMs}
\usage{
glmBayes(formula, data, family = "gaussian", log_lik = FALSE,
  iter = 10000, warmup = 1000, adapt = 2000, chains = 4,
  thin = 1, method = "parallel", cl = makeCluster(2), ...)
}
\arguments{
\item{formula}{the model formula}

\item{data}{a data frame}

\item{family}{one of "gaussian", "binomial", or "poisson".}

\item{log_lik}{Should the log likelihood be monitored? The default is FALSE.}

\item{iter}{How many post-warmup samples? Defaults to 10000.}

\item{warmup}{How many warmup samples? Defaults to 1000.}

\item{adapt}{How many adaptation steps? Defaults to 2000.}

\item{chains}{How many chains? Defaults to 4.}

\item{thin}{Thinning interval. Defaults to 1.}

\item{method}{Defaults to "parallel". For an alternative parallel option, choose "rjparallel" or. Otherwise, "rjags" (single core run).}

\item{cl}{Use parallel::makeCluster(# clusters) to specify clusters for the parallel methods. Defaults to two cores.}

\item{...}{Other arguments to run.jags.}
}
\value{
A run.jags object
}
\description{
This model utilizes normal-gamma mixture priors. A student-t prior can be parameterized as a norma-gamma mixture 
by utilizing a gamma(df/2, df/2) distribution on the precision, where df is the desired degrees of freedom. Smaller
degrees of freedom 
result in long tails which can capture larger coefficients with sufficient evidence while also providing regularization
for coefficients which lack strong evidence of being large. This prevents unrealistically large coefficient estimates
due to collinearity. Collinearity results in increased posterior variance due to a flatter likelihood function, and a side
effect of this is inflated coefficient estimates. \cr
\cr
This model utilizes a prior on the degrees of freedom, meaning the prior distribution is a mixture of student-t distributions
with different degrees of freedom. This setup yields a prior that is data-driven and adaptive. In other words, only as much
regularization is applied as is needed. The behavior of this estimator is nearly identical to ridge regression. \cr
\cr
Standard gaussian, binomial, and poisson likelihood functions are available. \cr
\cr
For an alternative prior that also performs very well see \code{\link[Bayezilla]{apcGlm}}
\cr
Note that if you do not scale and center your numeric predictors, this will likely not perform well or
give reasonable results. The mixing hyperparameters assume all covariates are on the same scale.
}
\examples{
glmBayes()

}
\seealso{
\code{\link[Bayezilla]{apcGlm}}
}
