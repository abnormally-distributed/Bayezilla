% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/glm_bayes.R
\name{glmBayes}
\alias{glmBayes}
\title{Bayesian Basic GLMs}
\usage{
glmBayes(formula, data, family = "gaussian", log_lik = FALSE,
  iter = 10000, warmup = 1000, adapt = 2000, chains = 4,
  thin = 1, method = "parallel", cl = makeCluster(2), ...)
}
\arguments{
\item{formula}{the model formula}

\item{data}{a data frame}

\item{family}{one of "gaussian", "binomial", or "poisson".}

\item{log_lik}{Should the log likelihood be monitored? The default is FALSE.}

\item{iter}{How many post-warmup samples? Defaults to 10000.}

\item{warmup}{How many warmup samples? Defaults to 1000.}

\item{adapt}{How many adaptation steps? Defaults to 2000.}

\item{chains}{How many chains? Defaults to 4.}

\item{thin}{Thinning interval. Defaults to 1.}

\item{method}{Defaults to "parallel". For an alternative parallel option, choose "rjparallel" or. Otherwise, "rjags" (single core run).}

\item{cl}{Use parallel::makeCluster(# clusters) to specify clusters for the parallel methods. Defaults to two cores.}

\item{...}{Other arguments to run.jags.}

\item{nu}{the degrees of freedom for the marginal student-t priors on the coefficients}
}
\value{
A run.jags object
}
\description{
This model utilizes normal-gamma mixture priors. A student-t prior can be parameterized as a norma-gamma mixture 
by utilizing a gamma(nu/2, nu/2) distribution where nu is the desired degrees of freedom. By default,
this model utilizes a single degree of freedom. One degree of freedom yields gamma(.5, .5), which gives marginal cauchy distributions. Hence, 
this model results in  marginal independent cauchy priors on each coefficient. The cauchy distribution has no defined 
first or second moments (mean and variance) and hence is an ideal proper reference prior. The cauchy distribution's 
extremely long tails allow coefficients with strong evidence of being large to not be shrunk too strongly, while the 
large probability mass at the mode of zero captures small noisy coefficients and regularizes them. However,
at times the cauchy distribution can be difficult to sample from, so I have enabled the user to select from either 1, 3, 6,
or 12 degrees of freedom. Larger degrees of freedom result in stronger regularization. Essentially, the degrees
of freedom controls the prior values of the omega hyperparameter (precision on the coefficient priors). Nevertheless,
this is only a *prior*. The model will learn from the data, and this adaptive shrinkage property results in an ideal prior
that, other than subjective choice of selecting the degrees of freedom, is completely data driven. However, one could always
usem mode comparison to select a degrees of freedom.
}
\details{
Standard gaussian, binomial, and poisson likelihood functions are available.

Note that if you do not scale and center your numeric predictors, this will likely not perform well or
give reasonable results. The mixing hyperparameter omega assumes all covariates are on the same scale.
}
\examples{
glmBayes()

}
