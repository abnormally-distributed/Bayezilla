% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/zsSpike.R
\name{zsSpike}
\alias{zsSpike}
\title{Zellner-Siow g-prior Stochasitc Search Variable Selection}
\usage{
zsSpike(formula, data, family = "gaussian", log_lik = FALSE,
  iter = 10000, warmup = 1000, adapt = 5000, chains = 4,
  thin = 1, method = "rjparallel", cl = makeCluster(2), ...)
}
\arguments{
\item{formula}{the model formula}

\item{data}{a data frame}

\item{family}{one of "gaussian", "binomial", or "poisson".}

\item{log_lik}{Should the log likelihood be monitored? The default is FALSE.}

\item{iter}{How many post-warmup samples? Defaults to 10000.}

\item{warmup}{How many warmup samples? Defaults to 1000.}

\item{adapt}{How many adaptation steps? Defaults to 2000.}

\item{chains}{How many chains? Defaults to 4.}

\item{thin}{Thinning interval. Defaults to 1.}

\item{method}{Defaults to "rjparallel". For an alternative parallel option, choose "parallel". Otherwise, "rjags" (single core run).}

\item{cl}{Use parallel::makeCluster(# clusters) to specify clusters for the parallel methods. Defaults to two cores.}

\item{...}{Other arguments to run.jags.}
}
\value{
A run.jags object.
}
\description{
IMPORTANT: I suggest not using any factor predictor variables, only numeric. In my experience the inclusion of categorical
predictors tends to lead to odd results in calculating the prior scale due to dummy variables being binary. \cr
\cr
The Zellner-Siow cauchy g-prior utilizes the inverse crossproduct is to determine the proper scale of the coefficient priors 
by treating the inverse crossproduct of the model matrix as a covariance matrix for a multivariate normal prior distribution 
for the coefficients, which is scaled by the parameter "g". The logic is that variables which carry the most information will
consequently have a more dispersed prior, while variables that carry less information will have priors more concentrated about zero. 
While the joint prior is multivariate normal, the implied independent marginal priors are Cauchy distributions with 
squared-scale equal to g*sigma2. The approach here is to let g be a random variable estimated as part of the model, rather than
fixed values of g=N. This avoids several problems associated with fixed-g priors. For more information, see Liang et al. (2008). \cr
\cr
The probability that a variable has no effect is 1 - mean(delta_i), where delta_i is an indicator variable that takes on values of 1 for
inclusion and 0 for exclusion. Averaging the number of 1s over the MCMC iterations gives the posterior inclusion probability (pip), hence, 
1-pip gives the posterior exclusion probability. The overall rate of inclusion for all variables is controlled by the hyperparameter 
"phi". Phi is given a beta(1,1) prior which gives uniform probability to the inclusion rate. \cr
\cr
The posterior means of the coefficients give the Bayesian Model Averaged estimates, which are the expected values of each 
parameter averaged over all sampled models (Hoeting et al., 1999). \cr
\cr
The model specification is given below. Note that the model formulae have been adjusted to reflect the fact that JAGS
parameterizes the normal and multivariate normal distributions by their precision, rater than (co)variance. For generalized
linear models plug-in pseudovariances are used.
\cr
Model Specification:
\cr
\cr
\if{html}{\figure{zsSpike.png}{}}
\if{latex}{\figure{zsSpike.png}{}}
\cr
\cr
Plugin Pseudo-Variances: \cr
\if{html}{\figure{pseudovar.png}{}}
\if{latex}{\figure{pseudovar.png}{}}
}
\examples{
zsSpike()

}
\references{
Zellner, A. & Siow S. (1980). Posterior odds ratio for selected regression hypotheses. In Bayesian statistics. Proc. 1st int. meeting (eds J. M. Bernardo, M. H. DeGroot, D. V. Lindley & A. F. M. Smith), 585–603. University Press, Valencia. \cr 
\cr
Zellner, A. (1986) On assessing prior distributions and Bayesian regression analysis with g-prior distributions. In P. K. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques: Essays in Honor of Bruno de Finetti, 233–243.  \cr
\cr
Kuo, L., & Mallick, B. (1998). Variable Selection for Regression Models. Sankhyā: The Indian Journal of Statistics, Series B, 60(1), 65-81. \cr
\cr
Hoeting, J., Madigan, D., Raftery, A. & Volinsky, C. (1999). Bayesian model averaging: a tutorial. Statistical Science 14 382–417. \cr
}
