---
title: "A Comparison of Glmnet and Bayesian Models."
output: html_notebook
---


```{r}
library(Bayezilla)
library(glmnetUtils)
library(ElemStatLearn)
data = scale(prostate)
data$train = NULL
```


```{r}
#L1.lambda = cv.glmnet(lpsa ~ ., data, alpha = 1)$lambda.min
#L2.lambda = cv.glmnet(lpsa ~ ., data, alpha = 0)$lambda.min
#EN.lambda = cv.glmnet(lpsa ~ ., data, alpha = 0.5)$lambda.min

OLS.beta = coef(lm(lpsa ~ ., data))
L1.beta = coef(glmnet(lpsa ~ ., data, alpha = 1, lambda = 0.01887591))[,1]
L2.beta = coef(glmnet(lpsa ~ ., data, alpha = 0, lambda = 0.07306646))[,1]
EN.beta = coef(glmnet(lpsa ~ ., data, alpha = 0.5, lambda = 0.02023775))[,1]
```


```{r}
data.frame("OLS" = OLS.beta, "LASSO" = L1.beta, "Ridge" = L2.beta, "Elastic Net" = EN.beta) %>% rownames_to_column() %>% mutate_if(is.numeric, round, 2)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
BL1 <- blasso(lpsa ~ ., data, iter = 10000, adapt = 5000, warmup = 5000)
BL2 <- glmBayes(lpsa ~ ., data, iter = 10000, adapt = 5000, warmup = 5000)
BEN <- bayesEnet(lpsa ~ ., data, iter = 10000, adapt = 5000, warmup = 5000)
```


```{r}
post_summary(BL1, keeppars = c("Intercept", "beta", "sigma"), digits = 2)
post_summary(BL2, keeppars = c("Intercept", "beta", "sigma"), digits = 2)
post_summary(BEN, keeppars = c("Intercept", "beta", "sigma"), digits = 2)
```

## Other Bayesian LASSO variants 

```{r message=FALSE, warning=FALSE, include=FALSE}
BL1a <- adaLASSO(lpsa ~ .,data, iter = 10000, thin = 3, adapt = 5000, warmup = 7500)
BL1b <- extLASSO(lpsa ~ ., data, iter = 10000, thin = 2, adapt = 5000, warmup = 5000)
BL1c <- negLASSO(lpsa ~ ., data, iter = 10000, thin = 2, adapt = 5000, warmup = 5000)
```

```{r}
post_summary(BL1a, keeppars = c("Intercept", "beta", "sigma"), digits = 2)
post_summary(BL1b, keeppars = c("Intercept", "beta", "sigma"), digits = 2)
post_summary(BL1c, keeppars = c("Intercept", "beta", "sigma"), digits = 2)
```

## Horseshoe Estimators

The Horseshoe is another innovation meant to address some of the perceived shortcomings of the Bayesian Lasso. The original Horseshoe was proposed by Carvalho et al. (2010) and it has been extremely well received within the Bayesian community. The horseshoe estimator aims to get at non-convex, sub-L1 norm penalities. After all, the L1 norm which the LASSO utilizes for estimation of coefficients is a convex relaxation of the L0 norm. The L0 norm is what we *really* want, which is the simple identification of which coefficients have no effect, or an effect too small or variable to be of use in building a good model. 

```{r}
hs1 = HS(lpsa ~ ., data, iter = 10000, adapt = 5000, warmup = 5000)
hs2 = HSplus(lpsa ~ ., data, iter = 10000, adapt = 5000, warmup = 5000)
hs3 = HSreg(lpsa ~ ., data, iter = 10000, adapt = 5000, warmup = 5000)
```


```{r}
post_summary(hs1, keeppars = c("Intercept", "beta", "sigma"), digits = 2)
post_summary(hs2, keeppars = c("Intercept", "beta", "sigma"), digits = 2)
post_summary(hs3, keeppars = c("Intercept", "beta", "sigma"), digits = 2)
```

## The Zellner-Siow Cauchy and Adaptive Powered Correlation Priors




## Stochastic Search Variable Selection (SSVS)

Another approach to Bayesian variable selection is a family of approaches known as stochastic search variable selection (SSVS). It is also known as Bayesian model averaging (BMA). 

In a stochastic search the $\beta_s$ are replaced by a step function, $f(\theta_j , \delta_j) = \theta_j \cdot \delta_j$, where $\delta$ is an indicator variable that takes on values of 0 or 1 and $\theta$ is a raw coefficient estimate.  
 
$$
\hat{y} = \text{Intercept} \ + \ f(\theta_j, \delta_j) 
$$

In a stochastic search the model explores two aspects of uncertainty at once. 

One is the presence or absence of an effect. In other words, a stochastic search explores the *model space*. This can also be understood as imposing an $L_0$ "norm" penalty on the coefficients, which continuous shrinkage methods such as the LASSO and Horseshoe approximate through $L_1$ and $L_{ \ 1 \leq \ p \ >0}$ penalties.

The other aspect  uncertainty in the parameter value, as is typical for a Bayesian model. The coefficients are given prior distributions and updated with the data through the likelihood function to yield a posterior distribution. This is an important feature, because frequentist methods such as best subsets and stepwise regression also attempt to utilize an $L_0$ "norm", with a predictor being either present or absent in a final model that is used for inference. Such methods have been extremely heavily criticized, and despite the continued popularity among researchers in the biomedical and social sciences, statisticians generally recommend best subsets and stepwise methods *not* be used for inference, only for prediction. 

By simulteously exploring the *parameter space*, a stochastic search can explore models with the same active set of predictors while varying the possible values of the coefficients. By accounting for both sources of uncertainty at once the bias in the inferences from frequentist $L_0$ regression methods is avoided. 

This approach is also called *Bayesian model averaging* because the posterior distribution is a mixture of point estimates from different models, such that the posterior mean is averaged over all explored models. Note that this does not mean exploring *all* possible models neccessarily. There are $2^p$ possible models in a variable selection problem, so even for 20 predictors there are 1048576 possible models. Rather than attempt this, the Bayesian approach explores the "typical set" with markov chain monte carlo (MCMC) methods in order to compute expected values. Another benefit is that in forward stepwise methods the set of active predictors in one model in the $n^{th}$ iteration of the stepwise regression determines the set of possible predictors in all following models. This creates a garden of forking paths scenarios, which can lead to significant instability in the set of active predictors for the final "best" model. Stochastic search avoids this, because each model is independent of the others. They are not explored in any particular oder. 

The general structure of a stochastic search model is as follows:

$$ 
\begin{align*}
\theta_j &  \sim \mathcal{pdf}(0, \eta_0) \\
\delta_j &  \sim \mathcal{Bernoulli}(\phi) \\
\beta_j \ | \  \theta_j , \delta_j &  = \delta_j \ \cdot \ \theta_j 
\end{align*}
$$

Note that $\phi$ is the prior inclusion probability, often specified as a single fixed number such as 0.20 or 0.50, but a prior distribution can also be given to $phi$. In this package I use a non-informative Jeffrey's prior, $\phi \sim \mathcal{Beta}(.5, .5)$ such that the overall inclusion rate is completely data driven.

This approach also makes it easy to calculate inclusion probabilities for each coefficient. This is simultaneously a measure of variable importance and "statistical significance". The posterior inclusion probability is simply the average of the vector of posterior samples for the inclusion indicator corresponding to a predictor, $p(\beta_j \neq 0 \ | \ X) = \mathbb{E}(\delta_j)$

While conceptually this approach is infinitely better than best subsets and stepwise regression, it does suffer some practical issues. As was noted, the number of possible models grows at a rate of $2^p$. Since the MCMC algorithm is also exploring the parameter space simultaneously, this means as the set of candidate predictors grows the model fitting becomes very, very, slow. This means the typical set cannot be adequately explored in an amount of time most consider practical. This is one reason that continuous shrinkage priors have become the dominant variable selection method of choice among practicioners of Bayesian statistics. 

Nevertheless, the data set being used for demonstration here is pretty small so a demonstration of a few models is not a problem.

```{r}
s1 = Spike(lpsa ~ ., data)
s2 = apcSpike(lpsa ~ ., data, lambda = 1.5)
s3 = IAt(lpsa ~ ., data)
```


```{r}
post_summary(s1, keeppars = c("Intercept", "beta", "sigma"), digits = 2)
post_summary(s2, keeppars = c("Intercept", "beta", "sigma"), digits = 2)
post_summary(s3, keeppars = c("Intercept", "beta", "sigma"), digits = 2)
```


```{r}
imageSigns(s1)
imageSigns(s2)
imageSigns(s3)
```

