---
title: "Modern Regression Modeling Methods"
output:
  html_document:
    df_print: paged
    dev: 'CairoPNG'
---



```{r message=FALSE, warning=FALSE, include=FALSE}
library(Bayezilla)
```


Properties of OLS (or Maximum Likelihood Estimates more generally): 

Ordinary least squares is the best linear unbiased estimator (BLUE) of the coefficients in ordinary linear model, so long as it exists and is unique. "Best" means giving the lowest standard error for the coefficient(s). For example, least absolute deviations will have standard errors $\geq$ to those of OLS. OLS is asymptotically unbiased in that as $N \rightarrow \infty$ OLS will recover the true values of the coefficients. 

This sounds like the ideal estimator, but it is only ideal under ideal conditions. OLS assumes several conditions:

1. The relationship between the model parameters (coefficients) and the outcome variable is linear.
2. There is no multi-collinearity. 
3. Observations are independent (Independent and identically distributed)
4. Errors are homoscedastic
5. Errors are Gaussianly distributed.

**Asymptotically unbiased.** 

That means, so long as the five assumptions are met, as the sample size approaches *infinity*, the estimates (meaning both predicted y values and coefficient terms) are unbiased. MLE is said to be the best linear unbiased estimator, or "BLUE". However, this name implies (and many statisticians speak as if) the regression estimates fit by MLE are unbiased, but for a finite sample size, this will typically not be so. The five assumptions are rarely perfectly met, and often at least one of them is violated. 

In any situation where one of the above 5 assumptions is not met there are alternative estimation methods which have superior performance. 


## Robust Regression

The asymptotic breakdown point (ABP) of an estimator of the parameter $\theta$ is the percentage of data points that are outlying observations it takes to make an estimator give an inaccurate estimate $\hat \theta$. 

The finite-sample breakdown point (FBP) is the largest proportion of data points that can be arbitrarily replaced by outliers in resampling without making the estimate inaccurate, which provides a non-asymptotic measure of the robustness of an estimator. While the ABP is of interest in characterizing how fragile an estimator is with increasing N, the FBP allows assessment of the breakown point at a fixed sample size. Nevertheless, going forward the more general notion of the ABP will be what is meant by the term breakdown point due to its generality, whereas the FBP is specific to a data set. 

Rousseeuw and Yohai (1984) proved that OLS estimates have a breakdown point of 1/N meaning that **it takes only 1 observation to invalidate the OLS estimate of** $\hat \theta$. 

Furthermore, in regression it is not only outliers in the outcome $y$ that matter, but also those in the predictor variables $x_p$, which are called leverage points. Furthermore, unusual *combinations* of $y_i$ and ${x_p}_i$ can produce bivariate (or multivariate, as the case may be) outliers. Common strategies of trimming observations with a $z$-score of $\pm 2$ or $\pm 3$ make sense on the surface, but fail to address multivariate outliers. Furthermore, the z-scores of the observations change after trimming, which can yield more observations that ought to be trimmed. Since this is a rule of thumb, there's no formal rule on when to stop. In addition, some analysts feel that data is data, and if it is not *known* if an observation is a mismeasurement or data entry error, it ought to be left alone.


So, if the Gaussian distribution fails so very easily, and removing subjectively judged outliers is questionable, what can be done? 

There are three solutions commonly employed: 

  1 ) The use of L1 norm loss functions, which results in estimating the median rather than mean. The corresponding regression estimator is the least absolute deviations (LAD) estimator in place of ordinary least squares (OLS). 

  2 ) The use of a mixture model where the means of the two distributions are identical, but the variances differ. This is referred to as the contaminated normal model. 

  3 ) The use of heavy tailed distributions, such as the Student-t distribution with a low degrees of freedom (1 - 5 generally to obtain sufficienty heavy tails).


The first solution has some benefits and disadvantages alike, summarized below.

|                                Mean Regression                                	|                                              Median Regression                                             	|
|:-----------------------------------------------------------------------------:	|:----------------------------------------------------------------------------------------------------------:	|
| Predicts the conditional mean E[y \| X]                                        	| Predicts conditional median, Q50[y \| X]                                                                    	|
| Applies when n is small                                                       	| Often requires larger sample size for comparable statistical efficiency (in regards to hypothesis testing) 	|
| Assumes normal, heteroskedastic, residuals                                    	| Does not care about the form of the residuals                                                              	|
| Does not preserve the relationship between y and X under data transformations 	| Preserves relationship between y and X under data transformations                                          	|
| Beakdown point of 1/n                                                         	| Breakdown point of 0.50                                                                                    	|
| Unique solution so long as design matrix X is not singular                    	| Can have multiple solutions even without a singular design matrix                                          	|


The second approach models the vector $y$ as consisting of a mixture of two
distributions with the same central location parameter. The classical notation is Tukey's "g and h" distribution, which models the likelihood of $y$ as a weighted function of two probability density functions denoted $\mathcal{G}$ and $\mathcal{H}$. 

$$
\mathcal{f}(y) = \mathcal{w}  \cdot \mathcal{G}(y) \ + \ (1 - \mathcal{w}) \cdot \mathcal{H}(y)
$$

The standard case is a mixture of two normal distributions each with the same mean, but different variances: 

$$
\mathcal{f}(y) =  \frac { w }{ \sqrt { 2 \pi \sigma ^ { 2 } } } \exp \left( - \frac { 1 } { 2 \sigma ^ { 2 } } ( y - \mu ) ^ { 2 } \right) \ + \  \frac { 1 - w } { \sqrt { 2 \pi \zeta ^ { 2 } } } \exp \left( - \frac { 1 } { 2 \zeta ^ { 2 } } ( y - \mu ) ^ { 2 } \right)
$$

One exception is the Huber density, which is a combination of Gaussian and Laplace distributions:

$$
\mathcal{f}(y) =  \frac { w }{ \sqrt { 2 \pi \sigma ^ { 2 } } } \exp \left( - \frac { 1 } { 2 \sigma ^ { 2 } } ( y - \mu ) ^ { 2 } \right) \ + \  \frac { 1 - w } {2 \zeta} \exp \left( - \frac { 1 } { 2 \zeta}  \left|y - \mu \right| \right)
$$

These concepts can be boile down to the choice of a specific loss function. Typically, in robust statistics the robust
loss function is called a rho ($\rho$) function. $\rho$ functions have three characteristics:

$\rho(y)$ is a non decreasing function of $|y|$
$\rho(y = 0) = 0$
$\rho(y)$ is increasing for $y > 0$ such that $\rho(y) < \rho(\infty)$


If $\rho$ is differentiable, the first derivative yields the psi ($\psi$) function, $\psi (y) = \frac { \text{d} \rho (y) } { \text{d} y }$.

Note that $\rho$ and $\psi$ need not correspond to a likelihood function. For example, the quadratic loss function corresponds to a Gaussian probability density function, and absolute loss to the Laplacian probability density function.

The $\psi$ function is used to generate weights via the weights function, $w(y_i) = \frac{\psi(y)}{y}$. These can be used as plug-ins for the $w_i$ in a Tukey's g and h mixture model, or as weights for the residuals. 

This general approach is called **M-Estimation** (or MM-Estimation when referring to the second generation of M-Estimators. The distinction will not be considered here.), which is an extension of Maximum Likelihood Estimation to accomodate likelihoods that combine the typical likelihood with a $\psi$ or $\rho$ function. Thus, the need to adopt a mixture model as for Tukey's $\mathcal{G \ \& \ H}$ is bypassed in favor of one of the two general strategies shown below:


\begin{align*}
& \text{Minimizing} \ \rho : & \mathscr{arg \ min}  & \ \sum _ { i = 1 } ^ { n } \rho \left( y _ { i } , \ \hat{y _ { i }} \right) & 
\\ & &  & = \mathscr{arg \ min} \ \sum _ { i = 1 } ^ { n } \mathcal{w_i} \cdot \mathbf{L}\left( y _ { i } , \ \hat{y _ { i }} \right) & 

\\

& \text{Solving } \psi \text{ for zero} :  & &  \sum _ { i = 1 } ^ { n } \psi \left( x _ { i } \right) = 0
\end{align*}

Below are the $\psi$, $\rho$, and weight functions for two commonly used robust approaches, Huber's $\psi$ and Tukey's bisquare functions. The $\rho$ functions have the Gaussian (the curved u-shaped function) and Laplacian (the v-shaped function) overlaid in dotted lines. Note that the Gaussian loss function has been scaled to the level of the others for the sake of visualization, since the natural scale is the square of the value along the x-axis.

<center>
```{r echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE, fig.height = 5.5, fig.width = 7.5}
huber.wts = function (r, c = 1.345){
    R <- abs(r)
    Rgtc <- R > c
    w <- r
    w[!Rgtc] <- 1
    w[Rgtc] <- c/R[Rgtc]
    return(w)
  }
  
tukey.wts = function (r, t = 4.685){
    return((1 - pmin(1, abs(r/t))^2)^2)
}

huber.psi = function(x){
  huber.wts(x) * x
}

tukey.psi = function(x){
  tukey.wts(x) * x
}

huber.rho = 
function(x){ifelse(abs(x) / 1.345 <= 1, x^2 / 2, (1.345*abs(x)) - (1.345^2 / 2))}

tukey.rho = 
function(x){5.782171 * ifelse(abs(x)  / 4.685 <= 1, 1-(1-(abs(x)/4.685)^2)^3, 1)}

h1 = plotFunc(huber.psi, from = -6, to = 6, ylab = paste0("\u03C8", "( ", "y", " )\n"), xlab = "y", size = 1.125, color = "blue") 

t1 = plotFunc(tukey.psi, from = -6, to = 6, ylab = paste0("\u03C8", "( ", "y", " )\n"), xlab = "y", size = 1.125, color = "red")

h2 = plotFunc(huber.wts, from = -6, to = 6, ylab = paste0("w", "(", "y", ")\n"), xlab = "y", size = 1.125, color = "blue") 

t2 = plotFunc(tukey.wts, from = -6, to = 6, ylab = paste0("w", "(", "y", ")\n"), xlab = "y", size = 1.125, color = "red") 

h3 = plotFunc(huber.rho, from = -6, to = 6, ylab = paste0("\u03C1", "(", "y", ")\n"), size = 1.125, xlab = "y" , color = "blue") + stat_function(fun = function(x){abs(x)}, linetype = 'dotdash', color = "#ef7b18", alpha = .75) + stat_function(fun = function(x){x^2 / 5.024082}, linetype = 'dashed', color = "#98777b", alpha = .65)

t3 = plotFunc(tukey.rho, from = -6, to = 6, ylab = paste0("\u03C1", "(", "y", ")\n"), xlab = "y", size = 1.125, color = "red") + stat_function(fun = function(x){abs(x)}, linetype = 'dotdash', color = "#ef7b18", alpha = .75) + stat_function(fun = function(x){x^2 / 6.226035}, linetype = 'dashed', color = "#98777b", alpha = .65)


gridExtra::grid.arrange(h3, t3, h1, t1, h2, t2)
```
</center>


The final way to deal with the outlier problem is to use a heavy tailed distribution such as Student's T distribution with a small degrees of freedom. 

```{r}
plotFunc(from = -6, to = 6, func = dst, args = list(nu = 3)) + stat_function(fun = dnorm, inherit.aes = TRUE, linetype = "dotted", size = 1.0125, color = "#cd5c5cCC")
```


This is a fully parametric approach to robustness, meaning it does not rely on 
a nonparametric function like the previous approaches. 



